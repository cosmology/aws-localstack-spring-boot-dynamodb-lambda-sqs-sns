x-default-logging: &logging
  driver: "json-file"
  options:
    max-size: "5m"
    max-file: "2"
    tag: "{{.Name}}"

networks:
  ls_network:
    ipam:
      config:
        - subnet: 10.0.2.0/24

services:

  # ***************************
  # AWS Cloud Services Emulator
  # ***************************
  localstack:
    container_name: "${LOCALSTACK_DOCKER_NAME:-localstack-main}"
    image: localstack/localstack:latest
    ports:
      - "127.0.0.1:4510-4559:4510-4559"
      - "127.0.0.1:4566:4566"
    environment:
      - AWS_DEFAULT_REGION=${AWS_REGION:-us-east-1}
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
      - SERVICES=${SERVICES}
      - DEBUG=${DEBUG:-0}
      - OTEL_COLLECTOR_HOST=${OTEL_COLLECTOR_HOST}
      - OTEL_COLLECTOR_PORT_HTTP=${OTEL_COLLECTOR_PORT_HTTP}
      - OTEL_EXPORTER_OTLP_ENDPOINT=http://otel-collector:4317
      - OTEL_EXPORTER_OTLP_METRICS_TEMPORALITY_PREFERENCE
      - OTEL_RESOURCE_ATTRIBUTES=${OTEL_RESOURCE_ATTRIBUTES}
      - OTEL_LOGS_EXPORTER=otlp
    volumes:
      - "$PWD/dynamodb-lambda-function/shared:/shared"
      - "$PWD/scripts/localstack/local-aws-infrastructure.sh:/etc/localstack/init/ready.d/init-aws.sh"
      - "$PWD/tmp/localstack:/var/lib/localstack"
      - "/var/run/docker.sock:/var/run/docker.sock"
    networks:
      ls_network:
        ipv4_address: 10.0.2.20

  # ******************
  # Core Demo Services
  # ******************

  # Ticket Producer Service
  ticket-producer:
    platform: linux/amd64
    container_name: ticket-producer
    hostname: ticket-producer
    build:
      context: ./ticket-producer
      dockerfile: Dockerfile
    environment:
      - TICKET_PRODUCER_RATE_LIMIT=${TICKET_PRODUCER_RATE_LIMIT}
      - TICKET_PRODUCER_RATE_DURATIONINMS=${TICKET_PRODUCER_RATE_DURATIONINMS}
      - AWS_REGION=${AWS_REGION}
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
      - OTEL_EXPORTER_OTLP_ENDPOINT=http://${OTEL_COLLECTOR_HOST}:${OTEL_COLLECTOR_PORT_HTTP}
      - OTEL_EXPORTER_OTLP_METRICS_TEMPORALITY_PREFERENCE
      - OTEL_RESOURCE_ATTRIBUTES=${OTEL_RESOURCE_ATTRIBUTES}
      - OTEL_LOGS_EXPORTER=otlp
      - OTEL_SERVICE_NAME=ticket-producer
      - OTEL_TRACES_EXPORTER=jaeger
      - OTEL_EXPORTER_JAEGER_ENDPOINT=http://jaeger:${OTEL_COLLECTOR_PORT_GRPC}
    depends_on:
      otel-collector:
        condition: service_started
    ports:
      - ${TICKET_PRODUCER_PORT}:${TICKET_PRODUCER_PORT}
    healthcheck:
      test:
        [
          "CMD-SHELL",
          "curl --fail --silent localhost:${TICKET_PRODUCER_PORT}/actuator/health | jq -e '.status == \"UP\"' > /dev/null || exit 1"
        ]
      interval: 5s
      timeout: 5s
      retries: 3
      start_period: 5s
    dns:
      - 10.0.2.20
    networks:
      - ls_network
    logging: *logging

  # Sport Ticket Consumer Service
  sport-consumer:
    platform: linux/amd64
    container_name: sport-consumer
    build:
      context: ./sport-ticket-consumer
      dockerfile: Dockerfile
    environment:
      - AWS_REGION=${AWS_REGION}
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
      - TICKET_PRODUCER_URL=${TICKET_PRODUCER_URL}
      - OTEL_EXPORTER_OTLP_ENDPOINT=http://${OTEL_COLLECTOR_HOST}:${OTEL_COLLECTOR_PORT_HTTP}
      - OTEL_EXPORTER_OTLP_METRICS_TEMPORALITY_PREFERENCE
      - OTEL_RESOURCE_ATTRIBUTES=${OTEL_RESOURCE_ATTRIBUTES}
      - OTEL_LOGS_EXPORTER=otlp
      - OTEL_SERVICE_NAME=sport-consumer
      - OTEL_TRACES_EXPORTER=jaeger
      - OTEL_EXPORTER_JAEGER_ENDPOINT=http://jaeger:${OTEL_COLLECTOR_PORT_GRPC}
    depends_on:
      ticket-producer:
        condition: service_healthy
    ports:
      - ${SPORT_CONSUMER_PORT}:${SPORT_CONSUMER_PORT}
    healthcheck:
        test:
          [
            "CMD-SHELL",
            "curl --fail --silent localhost:9081/actuator/health | jq -e '.status == \"UP\"' > /dev/null || exit 1"
          ]
        interval: 5s
        timeout: 5s
        retries: 3
        start_period: 5s
    dns:
      - 10.0.2.20
    networks:
      - ls_network
    logging: *logging

  # Movie Ticket Consumer Service
  movie-consumer:
    platform: linux/amd64
    container_name: movie-consumer
    build:
      context: ./movie-ticket-consumer
      dockerfile: Dockerfile
    environment:
      - AWS_REGION=${AWS_REGION}
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
      - TICKET_PRODUCER_URL=${TICKET_PRODUCER_URL}
      - OTEL_EXPORTER_OTLP_ENDPOINT=http://${OTEL_COLLECTOR_HOST}:${OTEL_COLLECTOR_PORT_HTTP}
      - OTEL_EXPORTER_OTLP_METRICS_TEMPORALITY_PREFERENCE
      - OTEL_RESOURCE_ATTRIBUTES=${OTEL_RESOURCE_ATTRIBUTES}
      - OTEL_LOGS_EXPORTER=otlp
      - OTEL_SERVICE_NAME=movie-consumer
      - OTEL_TRACES_EXPORTER=jaeger
      - OTEL_EXPORTER_JAEGER_ENDPOINT=http://jaeger:${OTEL_COLLECTOR_PORT_GRPC}
    depends_on:
      ticket-producer:
        condition: service_healthy
    ports:
      - ${MOVIE_CONSUMER_PORT}:${MOVIE_CONSUMER_PORT}
    dns:
      - 10.0.2.20
    networks:
      - ls_network
    logging: *logging

  # ********************
  # Telemetry Components
  # ********************

  # OpenTelemetry Collector
  otel-collector:
    image: ${COLLECTOR_CONTRIB_IMAGE}
    container_name: otel-collector
    restart: always
    command:
      - --config=/etc/otelcol-contrib/otel-collector.yaml
    volumes:
      - ${HOST_FILESYSTEM}:/hostfs:ro
      - ${DOCKER_SOCK}:/var/run/docker.sock:ro
      - ${OTEL_COLLECTOR_CONFIG}:/etc/otelcol-contrib/otel-collector.yaml
    ports:
      # - "1888:1888"                 # pprof extension
      # - "8888:8888"                 # Prometheus metrics exposed by the collector
      # - "8889:8889"                 # Prometheus exporter metrics
      - "13133:13133"                 # health_check extension
      # - "55679:55679"               # zpages extension
      - "${OTEL_COLLECTOR_PORT_GRPC}" # OTLP gRPC receiver
      - "${OTEL_COLLECTOR_PORT_HTTP}" # OTLP http receiver
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:13133"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 15s
    depends_on:
    #   # es01:
    #   #   condition: service_healthy
    #   # es02:
    #   #   condition: service_healthy
    #   # es03:
    #   #   condition: service_healthy
      jaeger:
        condition: service_healthy
    dns:
      - 10.0.2.20
    networks:
      - ls_network
    logging: *logging

  # Jaeger
  jaeger:
    image: ${JAEGERTRACING_IMAGE}
    container_name: jaeger
    command:
      - "--memory.max-traces=25000"
      - "--query.base-path=/jaeger/ui"
      - "--prometheus.server-url=http://${PROMETHEUS_ADDR}"
      - "--prometheus.query.normalize-calls=true"
      - "--prometheus.query.normalize-duration=true"
    deploy:
      resources:
        limits:
          memory: 1200M
    restart: unless-stopped
    ports:
      - "${JAEGER_SERVICE_PORT}:${JAEGER_SERVICE_PORT}"         # Jaeger UI
      - ${OTEL_COLLECTOR_PORT_GRPC}
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:16686"]
      interval: 10s
      timeout: 5s
      retries: 5
    environment:
      - COLLECTOR_OTLP_ENABLED=true
      - METRICS_STORAGE_TYPE=prometheus
    dns:
      - 10.0.2.20
    networks:
      - ls_network
    logging: *logging

  # Prometheus
  prometheus:
    image: ${PROMETHEUS_IMAGE}
    container_name: prometheus
    # extra_hosts: ['host.docker.internal:host-gateway']
    command:
      - --web.console.templates=/etc/prometheus/consoles
      - --web.console.libraries=/etc/prometheus/console_libraries
      - --storage.tsdb.retention.time=1h
      - --storage.tsdb.path=/prometheus
      - --web.enable-lifecycle
      - --web.enable-remote-write-receiver
      - --web.route-prefix=/
      - --enable-feature=exemplar-storage
      - --enable-feature=otlp-write-receiver
      - --config.file=/etc/prometheus/prometheus-config.yaml
    volumes:
      - ${PROMETHEUS_CONFIG}:/etc/prometheus/prometheus-config.yaml
    deploy:
      resources:
        limits:
          memory: 300M
    restart: always
    ports:
      - "${PROMETHEUS_SERVICE_PORT}:${PROMETHEUS_SERVICE_PORT}"
    healthcheck:
      test:
        ["CMD-SHELL","nc -z localhost ${PROMETHEUS_SERVICE_PORT}"]
    dns:
      - 10.0.2.20
    networks:
      - ls_network

  # Grafane
  grafana:
    image: ${GRAFANA_IMAGE}
    container_name: grafana
    deploy:
      resources:
        limits:
          memory: 100M
    restart: unless-stopped
    ports:
      - 3000:3000/tcp
    environment:
      - GF_AUTH_ANONYMOUS_ORG_ROLE=Admin
      - GF_AUTH_ANONYMOUS_ENABLED=true
      - GF_AUTH_BASIC_ENABLED=false
      - GF_FEATURE_TOGGLES_ENABLE=accessControlOnCall lokiLogsDataplane exploreLogsShardSplitting
      - GF_INSTALL_PLUGINS=https://storage.googleapis.com/integration-artifacts/grafana-lokiexplore-app/grafana-lokiexplore-app-latest.zip;grafana-lokiexplore-app
    volumes:
      - ./docker/grafana/grafana.ini:/etc/grafana/grafana.ini
      - ./docker/grafana/provisioning/:/etc/grafana/provisioning/
    depends_on:
      prometheus:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "nc -z localhost 3000"]
    dns:
      - 10.0.2.20
    networks:
      - ls_network
    logging: *logging

  # *********************
  # ELK Stack
  # *********************

  # Elasticsearch Cluster
  setup:
    container_name: setup
    image: docker.elastic.co/elasticsearch/elasticsearch:${STACK_VERSION}
    volumes:
      - certs:/usr/share/elasticsearch/config/certs
    user: "0"
    command: >
      bash -c '
        if [ x${ELASTIC_PASSWORD} == x ]; then
          echo "Set the ELASTIC_PASSWORD environment variable in the .env file";
          exit 1;
        elif [ x${KIBANA_PASSWORD} == x ]; then
          echo "Set the KIBANA_PASSWORD environment variable in the .env file";
          exit 1;
        fi;
        if [ ! -f config/certs/ca.zip ]; then
          echo "Creating CA";
          bin/elasticsearch-certutil ca --silent --pem -out config/certs/ca.zip;
          unzip config/certs/ca.zip -d config/certs;
        fi;
        if [ ! -f config/certs/certs.zip ]; then
          echo "Creating certs";
          echo -ne \
          "instances:\n"\
          "  - name: es01\n"\
          "    dns:\n"\
          "      - es01\n"\
          "      - localhost\n"\
          "    ip:\n"\
          "      - 127.0.0.1\n"\
          "  - name: es02\n"\
          "    dns:\n"\
          "      - es02\n"\
          "      - localhost\n"\
          "    ip:\n"\
          "      - 127.0.0.1\n"\
          "  - name: es03\n"\
          "    dns:\n"\
          "      - es03\n"\
          "      - localhost\n"\
          "    ip:\n"\
          "      - 127.0.0.1\n"\
          > config/certs/instances.yml;
          bin/elasticsearch-certutil cert --silent --pem -out config/certs/certs.zip --in config/certs/instances.yml --ca-cert config/certs/ca/ca.crt --ca-key config/certs/ca/ca.key;
          unzip config/certs/certs.zip -d config/certs;
        fi;
        echo "Setting file permissions"
        chown -R root:root config/certs;
        find . -type d -exec chmod 755 \{\} \;;
        find . -type f -exec chmod 644 \{\} \;;
        echo "Waiting for Elasticsearch availability";
        until curl -s --cacert config/certs/ca/ca.crt https://es01:9200 | grep -q "missing authentication credentials"; do sleep 30; done;
        echo "Setting kibana_system password";
        until curl -s -X POST --cacert config/certs/ca/ca.crt -u "elastic:${ELASTIC_PASSWORD}" -H "Content-Type: application/json" https://es01:9200/_security/user/kibana_system/_password -d "{\"password\":\"${KIBANA_PASSWORD}\"}" | grep -q "^{}"; do sleep 10; done;
        echo "All done!";
      '
    healthcheck:
      test: ["CMD-SHELL", "[ -f config/certs/es01/es01.crt ]"]
      interval: 1s
      timeout: 5s
      retries: 120
    depends_on:
      localstack:
        condition: service_healthy

  es01:
    container_name: es01
    depends_on:
      setup:
        condition: service_healthy
    image: docker.elastic.co/elasticsearch/elasticsearch:${STACK_VERSION}
    volumes:
      - certs:/usr/share/elasticsearch/config/certs
      - esdata01:/usr/share/elasticsearch/data
    ports:
      - ${ES_PORT}:9200
    environment:
      - node.name=es01
      - cluster.name=${CLUSTER_NAME}
      - cluster.initial_master_nodes=es01,es02,es03
      - discovery.seed_hosts=es02,es03
      - ELASTIC_PASSWORD=${ELASTIC_PASSWORD}
      - bootstrap.memory_lock=true
      - xpack.security.enabled=true
      - xpack.security.http.ssl.enabled=true
      - xpack.security.http.ssl.key=certs/es01/es01.key
      - xpack.security.http.ssl.certificate=certs/es01/es01.crt
      - xpack.security.http.ssl.certificate_authorities=certs/ca/ca.crt
      - xpack.security.transport.ssl.enabled=true
      - xpack.security.transport.ssl.key=certs/es01/es01.key
      - xpack.security.transport.ssl.certificate=certs/es01/es01.crt
      - xpack.security.transport.ssl.certificate_authorities=certs/ca/ca.crt
      - xpack.security.transport.ssl.verification_mode=certificate
      - xpack.license.self_generated.type=${LICENSE}
      - xpack.ml.use_auto_machine_memory_percent=true
    mem_limit: ${MEM_LIMIT}
    ulimits:
      memlock:
        soft: -1
        hard: -1
    healthcheck:
      test:
        [
          "CMD-SHELL",
          "curl -s --cacert config/certs/ca/ca.crt https://localhost:9200 | grep -q 'missing authentication credentials'",
        ]
      interval: 10s
      timeout: 10s
      retries: 120

  es02:
    container_name: es02
    depends_on:
      - es01
    image: docker.elastic.co/elasticsearch/elasticsearch:${STACK_VERSION}
    volumes:
      - certs:/usr/share/elasticsearch/config/certs
      - esdata02:/usr/share/elasticsearch/data
    environment:
      - node.name=es02
      - cluster.name=${CLUSTER_NAME}
      - cluster.initial_master_nodes=es01,es02,es03
      - discovery.seed_hosts=es01,es03
      - ELASTIC_PASSWORD=${ELASTIC_PASSWORD}
      - bootstrap.memory_lock=true
      - xpack.security.enabled=true
      - xpack.security.http.ssl.enabled=true
      - xpack.security.http.ssl.key=certs/es02/es02.key
      - xpack.security.http.ssl.certificate=certs/es02/es02.crt
      - xpack.security.http.ssl.certificate_authorities=certs/ca/ca.crt
      - xpack.security.transport.ssl.enabled=true
      - xpack.security.transport.ssl.key=certs/es02/es02.key
      - xpack.security.transport.ssl.certificate=certs/es02/es02.crt
      - xpack.security.transport.ssl.certificate_authorities=certs/ca/ca.crt
      - xpack.security.transport.ssl.verification_mode=certificate
      - xpack.license.self_generated.type=${LICENSE}
      - xpack.ml.use_auto_machine_memory_percent=true
    mem_limit: ${MEM_LIMIT}
    ulimits:
      memlock:
        soft: -1
        hard: -1
    healthcheck:
      test:
        [
          "CMD-SHELL",
          "curl -s --cacert config/certs/ca/ca.crt https://localhost:9200 | grep -q 'missing authentication credentials'",
        ]
      interval: 10s
      timeout: 10s
      retries: 120

  es03:
    container_name: es03
    depends_on:
      - es02
    image: docker.elastic.co/elasticsearch/elasticsearch:${STACK_VERSION}
    volumes:
      - certs:/usr/share/elasticsearch/config/certs
      - esdata03:/usr/share/elasticsearch/data
    environment:
      - node.name=es03
      - cluster.name=${CLUSTER_NAME}
      - cluster.initial_master_nodes=es01,es02,es03
      - discovery.seed_hosts=es01,es02
      - ELASTIC_PASSWORD=${ELASTIC_PASSWORD}
      - bootstrap.memory_lock=true
      - xpack.security.enabled=true
      - xpack.security.http.ssl.enabled=true
      - xpack.security.http.ssl.key=certs/es03/es03.key
      - xpack.security.http.ssl.certificate=certs/es03/es03.crt
      - xpack.security.http.ssl.certificate_authorities=certs/ca/ca.crt
      - xpack.security.transport.ssl.enabled=true
      - xpack.security.transport.ssl.key=certs/es03/es03.key
      - xpack.security.transport.ssl.certificate=certs/es03/es03.crt
      - xpack.security.transport.ssl.certificate_authorities=certs/ca/ca.crt
      - xpack.security.transport.ssl.verification_mode=certificate
      - xpack.license.self_generated.type=${LICENSE}
      - xpack.ml.use_auto_machine_memory_percent=true
    mem_limit: ${MEM_LIMIT}
    ulimits:
      memlock:
        soft: -1
        hard: -1
    healthcheck:
      test:
        [
          "CMD-SHELL",
          "curl -s --cacert config/certs/ca/ca.crt https://localhost:9200 | grep -q 'missing authentication credentials'",
        ]
      interval: 10s
      timeout: 10s
      retries: 120

  logstash:
    container_name: logstash
    depends_on:
      kibana:
        condition: service_healthy
    image: docker.elastic.co/logstash/logstash:${STACK_VERSION}
    restart: unless-stopped
    ports:
      - "5044:5044"
    volumes:
      - certs:/usr/share/logstash/certs
      - logstashdata01:/usr/share/logstash/data
      - logstash:/logstash
      - ./docker/logstash/pipeline:/usr/share/logstash/pipeline/
    environment:
      - xpack.monitoring.enabled=false
      - ELASTIC_USER=elastic
      - ELASTIC_PASSWORD=${ELASTIC_PASSWORD}
      - ELASTIC_HOSTS=https://es01:9200
      - xpack.security.enabled=true
      - xpack.security.http.ssl.enabled=true
      - xpack.security.http.ssl.key=certs/es01/es01.key
      - xpack.security.http.ssl.certificate=certs/es01/es01.crt
      - xpack.security.http.ssl.certificate_authorities=certs/ca/ca.crt
      - xpack.security.transport.ssl.enabled=true
      - xpack.security.transport.ssl.key=certs/es01/es01.key
      - xpack.security.transport.ssl.certificate=certs/es01/es01.crt
      - xpack.security.transport.ssl.certificate_authorities=certs/ca/ca.crt
      - xpack.security.transport.ssl.verification_mode=certificate
      - xpack.license.self_generated.type=${LICENSE}
      - xpack.ml.use_auto_machine_memory_percent=true
    healthcheck:
      test:
        [
          "CMD-SHELL",
          "curl -f http://localhost:9600 || exit 1"
        ]

  kibana:
    container_name: kibana
    depends_on:
      es01:
        condition: service_healthy
      es02:
        condition: service_healthy
      es03:
        condition: service_healthy
    image: docker.elastic.co/kibana/kibana:${STACK_VERSION}
    volumes:
      - certs:/usr/share/kibana/config/certs
      - kibanadata:/usr/share/kibana/data
    ports:
      - ${KIBANA_PORT}:5601
    environment:
      - SERVER_NAME=kibana
      - ELASTICSEARCH_HOSTS=["https://es01:9200","https://es02:9200","https://es03:9200"]
      - ELASTICSEARCH_USERNAME=kibana_system
      - ELASTICSEARCH_PASSWORD=${KIBANA_PASSWORD}
      - ELASTICSEARCH_SSL_CERTIFICATEAUTHORITIES=config/certs/ca/ca.crt
      - XPACK_SECURITY_ENCRYPTIONKEY=${ENCRYPTION_KEY}
      - XPACK_ENCRYPTEDSAVEDOBJECTS_ENCRYPTIONKEY=${ENCRYPTION_KEY}
      - XPACK_REPORTING_ENCRYPTIONKEY=${ENCRYPTION_KEY}
      - xpack.reporting.roles.enabled=false
      - xpack.security.http.ssl.enabled=false
    mem_limit: ${MEM_LIMIT}
    healthcheck:
      test:
        [
          "CMD-SHELL",
          "curl -s -I http://localhost:5601 | grep -q 'HTTP/1.1 302 Found'",
        ]
      interval: 10s
      timeout: 10s
      retries: 120

  filebeat:
    container_name: filebeat
    depends_on:
      es01:
        condition: service_healthy
      es02:
        condition: service_healthy
      es03:
        condition: service_healthy
      logstash:
        condition: service_healthy
    build:
      context: ./docker/filebeat
      dockerfile: Dockerfile
    restart: unless-stopped
    user: root
    volumes:
      - /var/lib/docker:/var/lib/docker:ro
      - /var/run/docker.sock:/var/run/docker.sock
    environment:
      - xpack.monitoring.enabled=false
      - ELASTIC_USER=elastic
      - ELASTIC_PASSWORD=${ELASTIC_PASSWORD}
      - ELASTIC_HOSTS=https://es01:9200

  # Log and trace aggregation and searching
  # loki:
  #   image: grafana/loki:main-e9b6ce9
  #   container_name: loki
  #   volumes:
  #     - ./docker/loki/loki-config.yaml:/etc/loki/loki-config.yaml
  #   command: -config.file=/etc/loki/loki-config.yaml
  #   ports:
  #     - "3100:3100"

  # tempo:
  #   image: grafana/tempo:latest
  #   container_name: tempo
  #   hostname: tempo
  #   command: [ "-config.file=/etc/tempo.yaml" ]
  #   volumes:
  #     - ./docker/tempo/tempo.yaml:/etc/tempo.yaml
  #   ports:
  #     - "3200:3200"   # tempo
  #     - "4317"        # otlp grpc
  #   healthcheck:
  #     interval: 5s
  #     retries: 10
  #     test: wget --no-verbose --tries=1 --spider http://localhost:3200/status || exit 1

  # zipkin-all-in-one:
  #   image: openzipkin/zipkin:latest
  #   container_name: zipkin-all-in-one
  #   hostname: zipkin-all-in-one
  #   restart: always
  #   # Environment settings are defined here https://github.com/openzipkin/zipkin/blob/master/zipkin-server/README.md#environment-variables
  #   environment:
  #     - JAVA_OPTS=-Xms1024m -Xmx1024m -XX:+ExitOnOutOfMemoryError
  #   ports:
  #     # Port used for the Zipkin UI and HTTP Api
  #     - 9411:9411
  #   # Uncomment to enable debug logging
  #   command: --logging.level.zipkin2=DEBUG

volumes:
  certs:
    driver: local
  esdata01:
    driver: local
  esdata02:
    driver: local
  esdata03:
    driver: local
  kibanadata:
    driver: local
  logstash:
    driver: local
  logstashdata01:
    driver: local
  prometheus:
    driver: local
  tempodata:
    driver: local
